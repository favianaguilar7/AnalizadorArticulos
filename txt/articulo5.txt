F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Universidad AutÃ³noma de QuerÃ©taro
Facultad de InformÃ¡tica



C.U. a 20/10/2023

Doctorado en Ciencias de la ComputaciÃ³n

Me permito presentar ante el H. Consejo AcadÃ©mico de la Facultad de InformÃ¡tica la siguiente solicitud de titulaciÃ³n por tesis de acuerdo con la fracciÃ³n VII, artÃ­culos: 45-54 y 55-58 del reglamento de titulaciÃ³n vigente.

Nombre del Alumno: Kenneth MejÃ­a PÃ©rez No. de Expediente: 258807
No. CVU: 1077850
Correo electrÃ³nico: ickennethmp@gmail.com TelÃ©fono: 773 119 89 26
LÃ­nea de investigaciÃ³n sobre la cual se desarrollarÃ¡ el trabajo: VisiÃ³n por computadora y su procesamiento embebido
Responsable de la asesorÃ­a: Dra. Diana Margarita CÃ³rdova Esparza Co-director: Dr. Juan RamÃ³n Terven Salinas
Programas educativos que apoya: IngenierÃ­a en ComputaciÃ³n, IngenierÃ­a en Telecomunicaciones y Redes, MaestrÃ­a en Ciencias de la ComputaciÃ³n, Doctorado en Ciencias de la ComputaciÃ³n
LÃ­nea  de  investigaciÃ³n  del  responsable:  VisiÃ³n  por  computadora  Firma  de  aceptaciÃ³n:             
Dra.  Diana  Margarita  CÃ³rdova  Esparza  Firma  de  aceptaciÃ³n:               
Dr. Juan RamÃ³n Terven Salinas
NÃºmero de veces que se a sometido esta propuesta al H. Consejo AcadÃ©mico de la Facultad: Primera vez (X)   2da. ( )   MÃ¡s de 2 veces ( )
Semestre o Cuatrimestre actual del alumno:  3Â° Fecha de Ingreso al programa:  09/08/2022





1

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N



LINEAMIENTOS GENERALES
1.  Documentos PDF editable
2.  MÃ­nimo 18 cuartilla
3.  MÃ¡ximo 25 cuartillas incluyendo Ã­ndice y referencias
4.  Los anexos no forman parte de las 25 cuartillas
5.  Espaciado a 1.5
6.  PÃ¡rrafos justificados.
7.  Tipo y tamaÃ±o de letra: Arial o Times New Roman tamaÃ±o 12, en todo el documento
8.  En caso de usar palabras en latÃ­n y/o griego o en otro idioma diferentes al de la escritura de la tesis, utilizar cursivas.
9.  MÃ¡rgenes: izquierdo 4 cm, derecho 2 cm, inferior y superior 3 cm.
10. Citas y referencias bibliogrÃ¡ficas de acuerdo al formato APA 7
11. Utilizar sÃ³lo letra en negritas en los tÃ­tulos




I. DATOS GENERALES
â€¢  TÃ­tulo del proyecto: Reconocimiento automÃ¡tico de gestos y anÃ¡lisis querolÃ³gico para la interpretaciÃ³n de la Lengua de SeÃ±as Mexicana â€¢ Nombre del alumno responsable: Kenneth MejÃ­a PÃ©rez
â€¢ Colaboradores:
o Directora de tesis, Dra. Diana Margarita CÃ³rdova Esparza o
Codirector de tesis, Dr. Juan RamÃ³n Terven Salinas


â€¢  Centro o lugar donde se realiza investigaciÃ³n: Universidad AutÃ³noma de QuerÃ©taro, Facultad de InformÃ¡tica, Campus Juriquilla.
â€¢  Tipo de investigaciÃ³n bÃ¡sica, aplicada o tecnolÃ³gica: Aplicada.
â€¢  LÃ­nea de investigaciÃ³n sobre la cual se desarrollarÃ¡ el trabajo: VisiÃ³n por computadora y su procesamiento embebido






II. ANTECEDENTES Y/O FUNDAMENTACIÃ“N TEÃ“RICA

2

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

La comunicaciÃ³n es una herramienta fundamental para el acceso a la informaciÃ³n, la participaciÃ³n social y el desarrollo de la vida en comunidad. Entre las diferentes formas de comunicaciÃ³n, se destaca la expresiÃ³n oral como la mÃ¡s comÃºn e importante, en consecuencia, como lo mencionan los autores SerafÃ­n, M. y GonzÃ¡lez, R., (2011) si esta vÃ­a de comunicaciÃ³n se ve suprimida por condiciones mÃ©dicas se imposibilita la realizaciÃ³n social, influyendo en otros aspectos de la vida personal, como: el desarrollo educativo, profesional y humano. Las personas pertenecientes a la comunidad sorda presentan una dificultad para la comunicaciÃ³n oral en una o ambas direcciones (como receptores o emisores), por tal motivo la comunidad ha desarrollado su propia lengua, es decir la lengua de seÃ±as.
Desde inicios de la dÃ©cada pasada podemos encontrar trabajos en el estado del arte sobre desarrollo de sistemas para el reconocimiento automÃ¡tico de la LSM, como lo menciona SolÃ­s (2016), podemos agrupar estos trabajos por los mÃ©todos de obtenciÃ³n de datos en dos grupos, el grupo 1 son dispositivos electrÃ³nicos y sensores que se llevan puestos como prendas de vestir, los cuales capturan informaciÃ³n precisa sobre las manos, dedos y brazos, por otro lado, tenemos al grupo 2, los cuales son dispositivos de obtenciÃ³n de datos Ã³pticos como lo son cÃ¡maras de color o cÃ¡maras de profundidad, estos se apoyan de tÃ©cnicas de visiÃ³n artificial para clasificar las seÃ±as.
Ambos grupos presentan ventajas y desventajas uno respecto a otro, ya que por una parte, los sistemas pertenecientes al grupo 1 presentan una alta precisiÃ³n en la captura de datos del movimiento de las manos., como se puede observar en el trabajo de SaldaÃ±a-GonzÃ¡lez (2018) en el cual se crea un guante traductor para la LSM, en el cual se utilizan sensores electrÃ³nicos como lo son de flexiÃ³n acelerÃ³metro y giroscopio para obtener el nivel de inclinaciÃ³n, posiciÃ³n de la mano, nivel de flexiÃ³n de los dedos entre otras mediciones necesarias para clasificar las seÃ±as, sin embargo, como lo menciona SolÃ­s (2016) la desventaja que tienen estos dispositivos es que el usuario debe de llevarlos todo el momento al ejecutar la lengua de seÃ±as y esto afecta la naturalidad con la que se expresa.



3

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Por otra parte, los sistemas pertenecientes al segundo grupo cuentan con la ventaja de que el usuario pueda realizar las seÃ±as con mayor naturalidad al no necesitar llevar algÃºn dispositivo puesto consigo, sin embargo como desventaja en muchos casos, las cÃ¡maras de color y profundidad pueden llegar a presentar ruido durante la captura de los datos, por lo que para ello en la mayorÃ­a de casos se requiere capturar los datos en ambientes controlados, en donde se debe encontrar un escenario libre de objetos, con fondos de color sÃ³lido, ropa sin variaciones de color y en donde se encuentre Ãºnicamente el participante en escena mirando a la cÃ¡mara, Algunos ejemplos de esto son los trabajos de SolÃ­s (2015, 2016), Cervantes (2016) y MartÃ­nez (2016), este tipo de ambientes reducen las posibles aplicaciones en un ambiente natural.
Para el presente trabajo de investigaciÃ³n se ha optado por realizar el proyecto utilizando cÃ¡maras de profundidad como dispositivo de adquisiciÃ³n de datos por lo que a continuaciÃ³n se muestra una breve investigaciÃ³n documental sobre el estado del arte de trabajos de investigaciÃ³n similares pertenecientes al grupo 2.
Tabla 1
Dispositivos de adquisiciÃ³n de datos pertenecientes al grupo 2, CÃ¡maras convencionales

Una/
Autor,                                         Tasa de Modo de    dos   EstÃ¡tico/  Tipo de
referencia                            TÃ©cnica utilizada   reconocimient
adquisiciÃ³n  mano  dinÃ¡mico  seÃ±a
y aÃ±o                                           o
s




SolÃ­s et al. (2015)


CÃ¡mara   Una mano



EstÃ¡tico  Letras
Preprocesamiento
: Momentos  de Jacobi-Fourier Clasificador: Redes neuronales



95%







4

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N




Cervante s et al. (2016)



CÃ¡mara


Dos mano s


EstÃ¡tico y dinÃ¡mico



Palabras
ObtenciÃ³n   de caracterÃ­sticas: algoritmo genÃ©tico. Clasificador: MÃ¡quina    de soporte



97%
vectorial
(SVM)












































5


F-50-01
TÃ©cnicas bÃ¡sicas de procesamiento de

MartÃ­nez
PROTOCOLO DE INVESTIGACIÃ“NimÃ¡genes.

et al. (2016)
CÃ¡mara   Una mano
EstÃ¡tico  Letras
IdentificaciÃ³n mediante   la comparaciÃ³n  de imÃ¡genes.
75%







SolÃ­s et   CÃ¡mara al. (2016)




Dos mano s





EstÃ¡tico  Letras
ObtenciÃ³n   de caracterÃ­sticas mediante momentos normalizados ClasificaciÃ³n:
Redes Neuronales artificiales   (ANN por sus siglas en inglÃ©s)





97%






PÃ©rez et   CÃ¡mara al. (2017)




Una  EstÃ¡tico  Letras mano
SegmentaciÃ³n: Fuzzy C-Means (FCM). ExtracciÃ³n de caracterÃ­sticas: Momentos de Hu.
ClasificaciÃ³n:
Redes    neuronales artificiales (ANN).




91%








Mancilla et al. (2019)







CÃ¡mara







Una  EstÃ¡tico  Letras mano
ExtracciÃ³n   de caracterÃ­sticas: Momentos de Hu, Momentos   de
Zernike     e Histogramas  de gradientes orientados (HOG). ClasificaciÃ³n: MÃ¡quina    de Soporte Vectorial (SVM)   Redes Neuronales Multicapa.







98.7%





MartÃ­nez- Seis et al. (2019)



CÃ¡mara (smartphone)



Una mano



EstÃ¡tico y dinÃ¡mico




Letras
DescripciÃ³n:
Canny y Camshift para seÃ±as
dinÃ¡micas. ClasificaciÃ³n: Redes neuronales convolucionales




92%

Fuente: CÃ³rdova-Esparza D.M., et Al. (2020).

Tabla 2
Dispositivos de adquisiciÃ³n de datos pertenecientes al grupo 2, CÃ¡maras de profundidad o RGB-D
6

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N



Autor, referenci a y aÃ±o





Galicia et al.
Modo de adquisiciÃ³ n
Una/ dos mano s





Amba
EstÃ¡tico/ dinÃ¡mic o

Tipo de seÃ±a

Tiemp o real

TÃ©cnica utilizada

ExtracciÃ³n de caracterÃ­stica
s:   Bosque aleatorio Clasificador: Redes

Tasa de reconocimient o

(2015)   Kinect
s   EstÃ¡tico  Letras   Si
neuronales    76.19%









Sosa- JimÃ©nez et al.
(2017)   Kinect









Amba s









DinÃ¡mic o









Palabras
/ frases   Si
Preprocesami ento de las imÃ¡genes: Filtro de color BinarizaciÃ³n ExtracciÃ³n de contorno Clasificador: Modelos
Ocultos   de Markov







Especificidad 80%
Sensibilidad 86%



GarcÃ­a- Bautista et al. [15]
(2017)   Kinect



Amba s



DinÃ¡mic
o   Palabras   Si
DeformaciÃ³n dinÃ¡mica del tiempo (DTW, por sus siglas
en inglÃ©s)     98.57%



JimÃ©nez,
J. et al. [16]
(2017)   Kinect   Una  EstÃ¡tico




Letras y nÃºmeros  No
ExtracciÃ³n de caracterÃ­stica s tipo Haar 3D. Clasificador: Adaboost





95%

ObtenciÃ³n de las coordenadas 3D  de
la

MartÃ­nez
-
GutiÃ©rrez
M. et al. [17]
(2019)



Intel Realsens
e f200   Una  EstÃ¡tico




Letras y palabras  No
mano. Clasificador: Red neuronal perceptrÃ³n multicapa





80.11%

Fuente: CÃ³rdova-Esparza D.M., et Al. (2020).


7

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

En total se reportaron 12 trabajos de reconocimiento de la LSM pertenecientes al grupo 2, de los cuales, 7 utilizan cÃ¡maras convencionales como medio de adquisiciÃ³n de datos, mientras que 5 utilizan cÃ¡maras de profundidad.

Como se puede interpretar de esta informaciÃ³n recopilada, existe una tendencia en la literatura entre el uso de modelos neuronales para la clasificaciÃ³n de la informaciÃ³n, por otra parte, en la literatura se encuentran diversas bases de datos que se utilizan para el reconocimiento automÃ¡tico del lenguaje de seÃ±as en diferentes  paÃ­ses  del  mundo  las  cuales  contienen  datos  sobre  personas expresÃ¡ndose en la respectiva lengua de seÃ±as de su paÃ­s, generalmente esta informaciÃ³n se presenta en forma de videos o imÃ¡genes RGB, y en algunas ocasiones incluyen algÃºn tipo de dato adicional, como etiquetas o distancias cuando se tienen imÃ¡genes de profundidad adquiridas con cÃ¡maras RGB-D.

Conjuntos de datos para el reconocimiento automÃ¡tico de las lenguas de seÃ±as en MÃ©xico y el mundo utilizando mÃ©todos de visiÃ³n por computadora
Tabla 3
Conjuntos de datos de acceso libre para el reconocimiento de diversas lengas de seÃ±as

Nombre de la Lengua de CategorÃ­a     Total, de  Perspecti  Dispositiv    Tipos de datos base de     seÃ±as     s       capturas  va      o    de adquiridos
datos y (SeÃ±as adquisiciÃ³ referencia distintas) n de
datos


RWTHPHOENIX-AlemÃ¡n    1,200    45,760   Personas    CÃ¡mara Videos     de

Weather. Forster
J. et
al, 2012
ejemplos
viendo de color hacia  el frente
resoluciÃ³n 210 x
260


PSL  Kinect 30
Kapuscinski T, 2015
Polaco    30     300
videos
Personas  Kinect viendo
hacia  el frente
Nubes de puntos Videos    de profundidad 320x240 px

PSL TOF 84
Polaco    84     1680
Personas CÃ¡mara con Video  de  176

Kapuscinski
videos
ropa ToF
x144 fotogramas
8

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

T, 2015                         negra
viendo hacia el frente
a       50
fotogramas/segu ndo Las nubes de puntos varÃ­an segÃºn el ejemplo

SIGNUM
Von Argis et. al, 2007

AlemÃ¡n   825  (45 seÃ±as bÃ¡sicas,7 80
oraciones
)

33,210
secuenci as  de video 5,970,45
0
imÃ¡gene s Aprox.
55.3 hrs de video

Personas viendo hacia el frente

CÃ¡mara de color

Video de color resoluciÃ³n 775x578 Imagen de color 24bpp


LSA64
Ronchetti F. et. al, 2016
Argentino   64     3,200 videos
Cuerpo de frente y uso de guantes de color
CÃ¡mara RGB
Videos de color resoluciÃ³n 1920x1080


ASLLVD
Neidle,  C, 2012
Americano  Mayor
3300
a MÃ¡s de 9800
videos
Cuerpo completo de frente y
pe rfil, enfoque en ambas manos
CÃ¡mara RGB
Videos de color 640 x 480 pixeles
2020 fotogramas por video


MS-ASL
Joze H. et. al, 2018
Americano  1,000    25,513 videos
Personas viendo hacia el frente
ObtenciÃ³ n   por
recortes de videos pÃºblicos
Procesamiento para ajustar a 24 x 24 pixeles


Rivas dataset Rivas-Perea al, 2019
Mexicano   10     3,000 imÃ¡gene s
Enfoque
en  una mano
Kinect V1  ImÃ¡genes   de
color 115x151 pixeles

Base   de
Mexicano   21     6,300
Enfoque
CÃ¡mara
ImÃ¡genes   de

datos Morales E. et al, 2019
imÃ¡gene s
en  una RGB mano
color 20 x 20 pixeles


ArASL
Arabe    32     54,049
Enfoque
CÃ¡mara
ImÃ¡genes

dataset Ghazanfar L. et. al, 2018
imÃ¡gene s
en  una RGB mano
en escala de grises 64 x 64 pixeles



9

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N


BosphorusSi gn22k Ã–zdemir
O,
2020
Turco     744     22,542
videos
Personas viendo hacia el frente
Kinect V2  Videos de color
1920x1080 pix,
30 FPS Videos de  profundidad 512x424 pixeles InformaciÃ³n de la posiciÃ³n de los cuerpos


How2Sign Duarte A. et. al, 2020
Americano  Mas  de
16,000
palabras
35,000
(83
horas de video)
Personas
CÃ¡mara de frente RGB  y y perfil  sensor de
profundid ad
Videos de color resoluciÃ³n 1280x720


WLASL
Americano  2,000
21,083
Personas
Videos y poses

Li, D et. al,
palabras
videos
CÃ¡maras viendo del esqueleto en

2020
de       2D
hacia  el profundid frente    ad y de
color

ASL-LEX  2.0Americano  2,723    2,723    Personas    CÃ¡mara Archivos    en

Sehyr Z. et. al, 2021
videos
viendo de color hacia  el frente
formato  webm, sin  informaciÃ³n de resoluciÃ³n

















Fuente: CÃ³rdova-Esparza D.M., et Al. (2020).
Estructura de la lengua de seÃ±as QuerologÃ­a:
La querologÃ­a es el tÃ©rmino de la teorÃ­a fonolÃ³gica usado para describir las unidades combinatorias elementales o queremas que constituyen las palabras y signos de las
10

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

lenguas de seÃ±as. La querologÃ­a puede considerarse una rama de la fonologÃ­a abstracta, El termino se acuÃ±o por William C. Stokoe en 1960, como lo describe Maher, J. (1966)
Los queremas son la equivalencia a los parÃ¡metros fonolÃ³gicos o fonemas de la lengua de seÃ±as, de acuerdo con Ibarra J. et al (2006) estos parÃ¡metros se pueden describir de la siguiente forma:
1. Queirema (configuraciÃ³n): ConfiguraciÃ³n manual de cada seÃ±a.
2. Kinema (movimiento): Tipo de movimiento de las manos (circular, zig zag, lineal, etc.)
3. Toponema (ubicaciÃ³n): UbicaciÃ³n en relaciÃ³n con el cuerpo.
4. Kineprosema (direcciÃ³n): DirecciÃ³n del movimiento de las manos.
5. Queirotropema (orientaciÃ³n): OrientaciÃ³n de la mano con respecto al cuerpo.
6. Prosoponema (rasgos no manuales): Todos aquellos rasgos que no utilizan las manos, principalmente movimiento corporal y expresiones faciales.









Fuente: MejÃ­a-PÃ©rez et.Al (2022)

La lengua de seÃ±as puede clasificarse por el uso de una o dos manos, esto se conoce como el uso de seÃ±as unimanuales o bimanuales respectivamente como se indica en el trabajo de Stark, T y Aldrete, M. C (2009), cada seÃ±ante tiene una mano base y una dominante, estas pueden alternarse entre izquierda y derecha y no pierde el sentido de la palabra o el significado.
Las seÃ±as unimanuales se pueden categorizar como estÃ¡ticas y dinÃ¡micas. Las seÃ±as estÃ¡ticas son aquellas que no requieren de movimiento para interpretarse, mientras que las dinÃ¡micas sÃ­; las seÃ±as bimanuales son dinÃ¡micas es decir, requieren el movimiento de la mano dominante, o incluso de la mano base, estas
11

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

seÃ±as a su vez pueden ser simÃ©tricas o asimÃ©tricas, esto se atribuye a que ambas manos tengan o no la misma configuraciÃ³n, a su vez las seÃ±as pueden contener movimiento simultaneo o alternado; las seÃ±as simultaneas son aquellas que tienen un movimiento inversamente proporcional, como si se tratara de un espejo. Por otra parte, las seÃ±as alternadas tienen movimientos idÃ©nticos inversos.

Redes neuronales
Dentro de los mÃ©todos mÃ¡s utilizados para la clasificaciÃ³n automÃ¡tica de seÃ±as son aquellos basados en redes neuronales, ademÃ¡s de ser muy utilizados, de acuerdo con la revisiÃ³n del estado arte son los que han aportado una mayor tasa de efectividad en la clasificaciÃ³n. Sin embargo, en su gran mayorÃ­a, estos modelos son implementados para seÃ±as estÃ¡ticas, es decir, son seÃ±as que no cambian a travÃ©s del tiempo, y de las cuales cada fotograma representa una Ãºnica unidad de informaciÃ³n, por lo que, para la clasificaciÃ³n de informaciÃ³n que cambia en el tiempo se requiere de un modelo distinto de red neuronal.
Enseguida se presenta una descripciÃ³n del modelo de perceptrÃ³n simple, lo que es equivalente a una neurona artificial clÃ¡sica, que es la base de los modelos neuronales existentes.
El modelo matemÃ¡tico-computacional de una neurona artificial recibe este nombre debido a que su estructura fue inspirada en los procesos sinÃ¡pticos de una neurona biolÃ³gica, Como se menciÃ³n en el trabajo Abdi,H (1994), En la dÃ©cada de 1960 el cientÃ­fico Frank Rosenblatt, creÃ³ el modelo del perceptrÃ³n, tomando base el trabajo de Warren McCulloch y Walter Pitts. Este modelo hace la analogÃ­a entre los procesos biolÃ³gicos de una neurona y los traduce a un modelo matemÃ¡tico que puede ser interpretado por una computadora, su estructura y equivalencia computacional se describe de la siguiente manera:
1. AxÃ³n de entrada â€“ Entradas de datos
2. Sinapsis â€“ Pesos de los datos
3. Cuerpo â€“ FunciÃ³n de aprendizaje y umbralizaciÃ³n de los datos
4. Cuello del axÃ³n â€“ FunciÃ³n de activaciÃ³n
5. AxÃ³n de salida â€“ Salida de datos


12

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

A continuaciÃ³n, se muestra en la Figura 1 el modelo de una neurona computacional. Figura 1. Modelo de una neurona computacional.












Fuente: MejÃ­a-PÃ©rez, K. (2022). (en que autor se basa)
En una red neuronal existe una cantidad ğ‘– de neuronas, denotadas como ğ‘µğ’Š Cada una de las neuronas recibe "ğ’â€œ cantidad de entradas simples denotados como vector de entrada (ğ’™ğ’ŠğŸ, ğ’™ğ’ŠğŸ â€¦ ğ’™ğ’Šğ’), este conjunto de datos es el mismo para cada entrenamiento, Cada uno de los valores entrada tiene un peso correspondiente denotado como ğ’˜ğ’Š, estos pesos determinan el nivel de importancia del dato que se estÃ¡ analizando, estos valores suelen iniciarse de forma aleatoria, este conjunto de datos varÃ­a entre cada neurona de la red.
Junto al conjunto de datos se incluye un valor de umbral ğ‘©ğ’Šğ’‚ğ’”, (ğ’™ğŸ ğ’ ğš¯ğ¢) quien suele tener el valor de -1 o 0, sin embargo, puede variar segÃºn el modelo de funciÃ³n de activaciÃ³n, este valor pude delimitar que tanto le creemos al resultado de la propagaciÃ³n. Los datos de entrada son tratados como un Ãºnico valor llamado â€œentrada globalâ€ y es denotado como ğ’ˆğ’Šğ’ğ’Š (global input). El siguiente paso es utilizar la funciÃ³n de sumatoria para procesar los datos de entrada, y se representa por la ec. (1).
ğ‘›

ğ‘”ğ‘–ğ‘›ğ‘–   (ğ‘¤ğ‘–ğ‘—ğ‘¥ğ‘–ğ‘—) + ğœ£ ec.(1)
ğ‘—





13

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Posteriormente se evalÃºa la funciÃ³n de activaciÃ³n, analÃ³gicamente una neurona biolÃ³gica puede estar activa (excitada) o inactiva (no excitada); es decir, que tiene un â€œestado de activaciÃ³n, la funciÃ³n activaciÃ³n calcula el estado de actividad de una neurona, transformando la sumatoria de entradas pesadas en un valor (estado) de activaciÃ³n, cuyo rango normalmente va de (0 ğ‘ 1) o de (âˆ’1 ğ‘ 1), esto es asÃ­, porque una neurona puede estar totalmente inactiva (0 ğ‘œ âˆ’ 1) o activa (1).
Dentro de los modelos de activaciÃ³n mÃ¡s frecuentes se encuentra la activaciÃ³n lineal, sigmoidea y el tangente hiperbÃ³lico (ğ‘¡ğ‘ğ‘›â„). La funciÃ³n de activaciÃ³n lineal se representa por la ec. (3).



ğ‘“



En donde:
â€¢  ğ‘¥ = ğ‘“(ğ‘”ğ‘–ğ‘›ğ‘–); ğ‘ > 0.
ğ‘

ğ‘    ğ‘

{    ğ‘¥  ğ‘

â€¢  ğ‘”ğ‘–ğ‘›ğ‘– es la entrada global

â€¢  ğ‘ es un valor arbitrario para su activaciÃ³n

La representaciÃ³n grÃ¡fica de la funciÃ³n de activaciÃ³n lineal se muestra en la Figura 2.

Figura 2: RepresentaciÃ³n grÃ¡fica de la activaciÃ³n lineal.













14

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N


Fuente: ElaboraciÃ³n propia.
La funciÃ³n de activaciÃ³n sigmoidea es denotada por la expresiÃ³n dada en la ec. (4):
1
ğœ(ğ‘¥) = 1     + ğ‘’âˆ’ğ‘”ğ‘¥

ec(4) En donde:

â€¢  ğ‘¥ = ğ‘“(ğ‘”ğ‘–ğ‘›ğ‘–)

â€¢  ğ‘” es un valor arbitrario que modifica la pendiente de la funciÃ³n de activaciÃ³n

Los valores de salida estÃ¡n comprendidos entre 0 y 1 y estÃ¡n representados grÃ¡ficamente de la siguiente forma (ver Figura 3):
Figura 3: RepresentaciÃ³n grÃ¡fica de la activaciÃ³n sigmoidea.












Fuente: ElaboraciÃ³n propia.
La funciÃ³n de tangente hiperbÃ³lica se denota por la ec. (5):
ğ‘’ğ‘”ğ‘¥ âˆ’ ğ‘’âˆ’ğ‘”ğ‘¥
ğ‘¡ğ‘ğ‘›â„(ğ‘¥) = ğ‘’       ğ‘”ğ‘¥ + ğ‘’âˆ’ğ‘”ğ‘¥  ec.(5)

Sea
â€¢  ğ‘¥ = ğ‘“(ğ‘”ğ‘–ğ‘›ğ‘–)

â€¢  g es un valor arbitrario que modifica la pendiente

En este caso los valores de salida estÃ¡n comprendidos entre -1 y 1, y la funciÃ³n de activaciÃ³n se comporta de la siguiente manera, ver Figura 4.

15

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Figura 4: RepresentaciÃ³n grÃ¡fica de la funciÃ³n de tangente hiperbÃ³lica.














Fuente: ElaboraciÃ³n propia.
Finalmente, para saber si la neurona aprendiÃ³ o no, se puede calcular el error restando la salida deseada ğ›¿ y la salida obtenida ğ‘¦, mediante la ec. (9):
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ = (ğ›¿ âˆ’ ğ‘¦) ec(9)

El Ãºltimo componente del modelo neuronal es la funciÃ³n de salida que indica el resultado de cada neurona ğ‘ğ‘–, esta funciÃ³n establece el valor se transfiere a las neuronas conexas, de tal modo que la salida de una neurona es una de las entradas de otra neurona, si la funciÃ³n de activaciÃ³n estÃ¡ por debajo del umbral determinado
ğ›©ğ‘–, ninguna salida se transfiere a la neurona subsiguiente.

Aprendizaje de una neurona
Para saber si una neurona estÃ¡ aprendiendo se puede comparar los resultados obtenidos en comparaciÃ³n del vector de salidas deseadas, de ser necesario se pueden volver a recalcular los pesos ğ‘¤, este proceso de aprendizaje puede utilizar una tasa de aprendizaje ğ›¼ que se encuentra entre 0 y 1 (0 < ğ›¼ < 1). Como se muestra en la ec. (10)


ğ‘¤(ğ‘—)
â€² = {ğ‘¤(ğ‘—) + ğ›¼(ğ›¿ âˆ’ ğ‘¦)ğ‘¥(ğ‘—) ec(10)

ğ‘¤(ğ‘—) + (ğ›¿ âˆ’ ğ‘¦)ğ‘¥(ğ‘—)


16

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N


PropagaciÃ³n hacia atrÃ¡s (Backpropagation):
Este es un modelo basado en el perceptrÃ³n multicapa, en donde se realiza el proceso de entrenamiento de la misma forma, sin embargo, la evaluaciÃ³n del error y actualizaciÃ³n de los pesos dependen de la salida final de la red, esto puede ser calculado como se muestra en la ec. (11).

ğ‘š

ğ¸ğ¶ğ‘€ = ğ¸(ğ‘¥; ğ‘Š1,ğ‘Š2)                .
ğ‘–


Tal que
â€¢  E es el Error CuadrÃ¡tico Medio (ECM)
â€¢  ğ‘Š1,ğ‘Š2,â€¦ , ğ‘Šğ¹ son las derivadas parciales del error con respecto a los pesos de cada capa, considerando este ejemplo para una red de 2 capas
â€¢  m la cantidad de neuronas en cada capa
â€¢  ğœ¹ salida deseada
â€¢  ğ²ğ¹ es la salida final obtenida

El perceptrÃ³n simple estÃ¡ definido por la estructura bÃ¡sica de una red neuronal, en donde se tienen datos de entrada y datos de salida, el perceptrÃ³n multicapa o red neuronal estÃ¡ definido por salidas de perceptrones simples interconectadas a las entradas de otros perceptrones simples como se muestra en la Figura 8.
Figura 8. Estructura de una red neuronal.














17

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N















Fuente: ElaboraciÃ³n propia.
Como se observa en la Figura 8, en la estructura de una red neuronal multicapa se encuentran cuatro componentes principales: la capa de entrada, la capa oculta, la capa de salida y finalmente la predicciÃ³n.
Capa de entrada: En esta capa se encuentran los datos que ingresaran a las neuronas para iniciar el proceso de aprendizaje.
Capa oculta: una vez que las primeras neuronas procesan los datos de entrada, sus salidas serÃ¡n conectadas a las entradas de nuevas neuronas, cabe destacar que pueden tener mÃ¡s de una capa oculta, segÃºn el modelo de la red neuronal y su aplicaciÃ³n.
Capa de salida: se encuentran los valores finales de la red neuronal, esta capa puede tener una Ãºnica salida o varias segÃºn el modelo requerido.
PredicciÃ³n: Es el resultado final de la red neuronal y puede calcularse por diversos mÃ©todos, el mÃ¡s comÃºn de ellos es simplemente obtener el valor mÃ¡ximo de todas las funciones de salida.

III. JUSTIFICACIÃ“N
En este trabajo de investigaciÃ³n se realizarÃ¡ el diseÃ±o e implementaciÃ³n de un sistema para el reconocimiento de la LSM que ofrece una propuesta de soluciÃ³n para tratar de reducir las barreras de comunicaciÃ³n de la comunidad sorda en MÃ©xico. Mediante el sistema es posible identificar un conjunto de palabras del Lenguaje de SeÃ±as Mexicano, haciendo uso de un algoritmo de aprendizaje
18

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

automÃ¡tico y una cÃ¡mara de profundidad (RGB-D) como medio de obtenciÃ³n de datos, ya que como se observa en el estado del arte es un dispositivo que permite obtener datos de profundidad que son invariantes a cambios de iluminaciÃ³n en la escena, color de piel y de ropa de la persona seÃ±ante, etc.


IV. DESCRIPCIÃ“N DEL PROBLEMA


En la actualidad las personas con discapacidades o deficiencias auditivas tienen barreras de comunicaciÃ³n en su vida cotidiana, siendo incluso imposible establecer una comunicaciÃ³n con aquellos que desconocen por completo el lenguaje de seÃ±as. De acuerdo con datos de la OMS (OMS, 2019) 466 millones de personas en el mundo padecen una discapacidad auditiva, de las cuales 34 millones son niÃ±os, en MÃ©xico existen alrededor de 498,640 personas con discapacidades auditivas (INEGI 2010), mientras que se estima un total de 87,000 a 100,00 usuarios de alguna lengua viso-gestual (ILV, 2014).

Como se describiÃ³ en las secciones anteriores, los trabajaos registrados en el estado del arte procuran reconocer un conjunto de datos obtenidos de imÃ¡genes de video o profundidad para realizar el proceso de aprendizaje, sin embargo el tratamiento de esta informaciÃ³n se centra en seccionar al participante del entorno en donde se encuentra y posteriormente tomar los datos del cuerpo o incluso Ãºnicamente de las manos para realizar la investigaciÃ³n, por otra parte, este proyecto propone describir la lengua de seÃ±as por medio de su estructura querolÃ³gica y utilizar estos parÃ¡metros como descriptores principales para el proceso de aprendizaje automÃ¡tico.

V. COMITÃ‰ DE BIOÃ‰TICA
Durante el desarrollo de este proyecto de investigaciÃ³n se contarÃ¡ con la participaciÃ³n de diferentes personas para la realizaciÃ³n de captura de los datos iniciales; Los datos personales proporcionados por los participantes serÃ¡n tomados como confidenciales y pertenecerÃ¡n anÃ³nimos durante el desarrollo de esta
19

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

investigaciÃ³n, sin embargo, podrÃ¡n tomarse datos generales de referencia como lo son edad, gÃ©nero, estatura entre otros

VI. PLANTEAMIENTO TEÃ“RICO (HipÃ³tesis y/o supuestos, pregunta, etc.) Preguntas de investigaciÃ³n
1. Â¿CuÃ¡l es la forma mÃ¡s eficiente de describir e interpretar la LSM?
2. Â¿CuÃ¡les tÃ©cnicas de reconocimiento automÃ¡tico nos permitirÃ¡n clasificar los queremas de la LSM?
3. Â¿Es posible clasificar los queremas de la LSM a travÃ©s de aprendizaje automÃ¡tico?
4. Â¿Es posible reconocer las seÃ±as de la LSM a travÃ©s de su descripciÃ³n querolÃ³gica?
HipÃ³tesis, Supuestos y/o proposiciones de investigaciÃ³n
â€¢  HipÃ³tesis: La caracterizaciÃ³n computacional de la lengua de seÃ±as a travÃ©s de la descripciÃ³n querolÃ³gica permitirÃ¡ reconocer un conjunto de seÃ±as de la lengua de seÃ±as mexicana.

VII.  OBJETIVOS Objetivo general:
Desarrollar una metodologÃ­a que permita reconocer de manera automÃ¡tica un conjunto de seÃ±as de la LSM con base en su descripciÃ³n querolÃ³gica, aplicando tÃ©cnicas de Inteligencia Artificial y VisiÃ³n por Computadora, como base de un motor para la traducciÃ³n de la lengua de seÃ±as.

Objetivos especÃ­ficos:
1. Capturar datos de la lengua de seÃ±as mexicana utilizando una cÃ¡mara de profundidad.
2. Realizar la caracterizaciÃ³n de queremas de la LSM utilizando algoritmos computacionales y/o tÃ©cnicas de inteligencia artificial.
3. Realizar la clasificaciÃ³n individual de los queremas pertenecientes a la LSM
4. Realizar la clasificaciÃ³n de un conjunto de palabras pertenecientes a la LSM utilizando la clasificaciÃ³n de queremas individuales.


20

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

VIII. METODOLOGÃA
Para la metodologÃ­a de desarrollo de este proyecto de investigaciÃ³n se secciono en 5 etapas: InvestigaciÃ³n documental, ObtenciÃ³n de los datos, Procesamiento de los datos, ExperimentaciÃ³n y AnÃ¡lisis de resultados.
1. InvestigaciÃ³n documental:
Esta es una tarea que se realizara continuamente a lo largo del proyecto, consiste en 2 subtareas principales
1.1  ActualizaciÃ³n del estado del arte.
1.2  Investigar modelos de clasificaciÃ³n.
2. ObtenciÃ³n de los datos:
2.1  Captura de los datos utilizando cÃ¡maras de profundidad
2.2  Realizar la descripciÃ³n querolÃ³gica de las seÃ±as capturadas
3. Procesamiento de los datos
3.1  CaracterizaciÃ³n de las imÃ¡genes de profundidad
3.2  Realizar la caracterizaciÃ³n de los queremas
4. ExperimentaciÃ³n
4.1  Clasificar individualmente los queremas
4.2  Clasificar las seÃ±as basÃ¡ndose en los queremas
5. AnÃ¡lisis de resultados
5.1  Probar los modelos aprendidos
5.2  Graficar resultados
6. ProducciÃ³n cientÃ­fica
6.1  Escritura de artÃ­culos cientÃ­ficos
6.2  PublicaciÃ³n de artÃ­culos cientÃ­ficos
6.3  ParticipaciÃ³n en actividades acadÃ©micas
6.4  Escritura de tesis

Diagrama de la metodologÃ­a de desarrollo:







21

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N













ProducciÃ³n cientifica
Escritura de artiuclos cientificos PublicaciÃ³n de articulos cientificos ParticipaciÃ³n en actividades academicas Escritura de tesis


























22

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Cronograma de actividades.




# 1.1
1Â° Semestre
2Â° Semestre
3Â° Semestre
4Â° Semestre
5Â° Semestre
6Â° Semestre
7Â° Semestre
8Â° Semestre
1.2

2.1

2.2

3.1

3.2

4.1

4.2

5.1

5.2

6.1

6.2

6.3

6.4



IX. RESULTADOS ESPERADOS, POSIBLES APLICACIONES Y USO DEL PROYECTO Como
resultado se pretendo obtener un mÃ©todo para la clasificaciÃ³n de lengua de seÃ±as mexicanas basado en la descripciÃ³n querolÃ³gica de la LSM, utilizando mÃ©todos computacionales de inteligencia artificial y visiÃ³n por computadora.
â€¢ Productos acadÃ©micos:
âˆ’ ParticipaciÃ³n en congreso internacional.
âˆ’ ArtÃ­culo en revista indexada JCR.

X. INDICE TENTATIVO DE LA TESIS
Dedicatoria Agradecimientos Ãndice
Ãndice de Cuadros Ãndice de Figuras

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Abreviaturas y siglas Resumen en espaÃ±ol Abstrac
IntroducciÃ³n
I.   Planteamiento del problema
II.  JustificaciÃ³n
III.  Antecedentes
IV.  HipÃ³tesis
V.  Objetivo
VI.  MetodologÃ­a
VII.  Resultados y discusiÃ³n
VIII.  Conclusiones
IX.  Referencias
XI. REFERENCIAS BIBLIOGRÃFICAS
Abdi, H. (1994). A neural network primer. Journal of Biological Systems, 2(03), 247-281.
Cervantes, J., GarcÃ­a-Lamont, F., RodrÃ­guez-Mazahua, L., Rendon, A. Y., & Chau, A. L. (2016). Recognition of Mexican sign language from frames in video sequences. In International Conference on Intelligent Computing, pp. 353-362.
Springer, Cham.
CÃ³rdova-Esparza D.M., et Al. (2020): Sistemas de reconocimiento automÃ¡tico de la Lengua de SeÃ±as Mexicana (LSM): RevisiÃ³n SistemÃ¡tica.
Duarte, A., Palaskar, S., Ghadiyaram, D., DeHaan, K., Metze, F., Torres, J., Giroi-Nieto, X. How2Sign: a large-scale multimodal dataset for continuous American sign language. arXiv preprint arXiv:2008.08143 (2020).
Forster, J., Schmidt, C., Hoyoux, T., Koller, O., Zelle, U., Piater, J. H., Ney, H. RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus. In LREC, Vol. 9, pp. 3785-3789 (2012)
Galicia, R., Carranza, O., JimÃ©nez, E. D., & Rivera, G. E. (2015). Mexican sign language recognition using movement sensor. IEEE 24th International Symposium on Industrial Electronics (ISIE), pp. 573-578.



24

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

GarcÃ­a-Bautista, G., Trujillo-Romero, F., & Caballero-Morales, S. O.: Mexican sign language recognition using kinect and data time warping algorithm. In 2017 International  Conference  on  Electronics,  Communications  and  Computers (CONIELECOMP) pp. 1-5. IEEE (2017).
Ghazanfar, L., Jaafar, A., Nazeeruddin, M., Roaa, A., Rawan, A. Arabic alphabets sign language dataset (arasl). Mendeley Data, v1 (2018)
Ibarra, J. L., Barba, M. D. R. R., & Picazo, V. T. (2006). SENSor foto-elÃ©ctrico aplicado al movimiento de los dedos de las manos. ComputaciÃ³n y Sistemas, 10(1), 57-68.
JimÃ©nez, J., Martin, A., Uc, V., & Espinosa, A.: Mexican sign language alphanumerical gestures recognition using 3D Haar-like features. IEEE Latin America Transactions, 15(10), 2000-2005 (2017).
Joze, H. R. V., Koller, O. Ms-asl: A large-scale data set and benchmark for understanding american sign language. arXiv preprint arXiv:1812.01053 (2018)
Kapuscinski, T., Oszust, M., Wysocki, M., Warchol, D. Recognition of hand gestures observed by depth cameras. International Journal of Advanced Robotic Systems, 12(4), 36 (2015)
Li, D., RodrÃ­guez, C., Yu, X., Li, H. Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1459-
1469 (2020)
Mancilla, E., VÃ¡zquez, O., Arguijo, P., MelÃ©ndez, R., & VÃ¡zquez, A.: TraducciÃ³n del lenguaje de seÃ±as usando visiÃ³n por computadora. Research in Computing Science, 148, 79- 89 (2019).
MartÃ­nez, M., Rojano-CÃ¡ceres, J., BÃ¡rcenas I., & JuÃ¡rez, F. (2016). IdentificaciÃ³n de lengua de seÃ±as mediante tÃ©cnicas de procesamiento de imÃ¡genes. Res. Comput. Sci., 128, 121-129.
MartÃ­nez-GutiÃ©rrez, M. E., Rojano-CÃ¡ceres, J. R., BenÃ­tez-Guerrero, E., & SÃ¡nchez-Barrera, H. E. Data Acquisition Software for Sign Language Recognition. Research in Computing Science, 148, 205-211 (2019).


25

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Martinez-Seis,  B.,  Pichardo-Lagunas,  O.,  RodrÃ­guez-Aguilar,  E.,  & SaucedoDÃ­az, E. R.: Identification of Static and Dynamic Signs of the Mexican Sign Language Alphabet for Smartphones using Deep Learning and Image Processing. Research in Computing Science, 148, 199-211 (2019).
MejÃ­a-PÃ©rez, K. et. Al,(2022) Automatic Recognition of Mexican Sign Language Using a Depth Camera and Recurrent Neural Networks.
MejÃ­a-PÃ©rez, K. (2022). Sistema de reconocimiento del Lenguaje de SeÃ±as Mexicano basado en una cÃ¡mara RGB-D y aprendizaje automÃ¡tico.
Morales, E. M., Aparicio, O. V., Arguijo, P., Armenta, R. A. M., LÃ³pez, A. H. V.
Â´ TraducciÃ³n del lenguaje de seÃ±as usando visiÃ³n por computadora. Research in Computing Science, 148, 79-89 (2019)
Maher, J. (1996). Seeing language in sign: The work of William C. Stokoe.
Gallaudet University Press.
Neidle, C., Thangali, A., Sclaroff, S. Challenges in development of the american sign language lexicon video dataset (asllvd) corpus. In 5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, LREC (2012)
Ozdemir, O., KÄ±ndÄ±roglu, A. A., Camgoz, N. C., Akarun, L. Bosphorussign22k sign
Â¨ language recognition dataset. arXiv preprint arXiv:2004.01283 (2020)  Rivas-Perea P. Desarrollo de un intÃ©rprete bÃ¡sico del lenguaje de seÃ±as para dactilologÃ­a empleando inteligencia artificial. TecnolÃ³gico Nacional De MÃ©xico,
Instituto TecnolÃ³gico De Nogales, https://www.reev.us/pdfs/rivas2019desarrollo.pdf (2019)
Ronchetti, F., Quiroga, F., Estrebou, C. A., Lanzarini, L. C., Rosete, A.
LSA64: an Argentinian sign language dataset. In XXII Congreso Argentino de Ciencias de la ComputaciÂ´on (CACIC 2016) (2016)
SaldaÃ±a GonzÃ¡lez, G., Cerezo SÃ¡nchez, J., Bustillo DÃ­az, M. M., & Ata PÃ©rez,
A. (2018). Recognition and classification of sign language for spanish. ComputaciÃ³n y Sistemas, 22(1), 271-277.
Sehyr, Z. S., Caselli, N., Cohen-Goldberg, A. M., Emmorey, K. The ASL- LEX 2.0 Project: A Database of Lexical and Phonological Properties for 2,723

26

F-50-01 PROTOCOLO DE INVESTIGACIÃ“N

Signs in American Sign Language. The Journal of Deaf Studies and Deaf Education, 26(2), 263-277 (2021)
SerafÃ­n, M., GonzÃ¡lez, R. (2011). Manos con voz, diccionario de lenguaje de seÃ±as mexicana. Consejo Nacional para Prevenir la DiscriminaciÃ³n, 15-19.
SolÃ­s, F., MartÃ­nez, D., & Espinoza, O. (2016). Automatic mexican sign language recognition using normalized moments and artificial neural networks.
Engineering, 8(10), 733-740.
SolÃ­s, F., Toxqui, C., & MartÃ­nez, D. (2015). Mexican sign language recognition using jacobifourier moments. Engineering, 7(10), 700.
Sosa-JimÃ©nez, C. O., RÃ­os-Figueroa, H. V., Rechy-RamÃ­rez, E. J., MarinHernandez, A., & GonzÃ¡lez-CosÃ­o, A. L. S.: Real-time mexican sign language recognition. In 2017 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC), pp. 1-6, IEEE (2017).
Stark, T. C. S., Aldrete, M. C. (2006). La morfologÃ­a en la Lengua de SeÃ±as mexicana. In Conferencia magistral preparada para el II Congreso Internacional de Logogenia MÃ©xico.
Von Agris, U., Kraiss, K. F. Towards a video corpus for signer-independent continuous sign language recognition. Gesture in Human-Computer Interaction and Simulation, Lisbon, Portugal (2007)






M.C. Kenneth MejÃ­a PÃ©rez


Nivel de revisiÃ³n: 02













27
